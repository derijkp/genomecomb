= Job system =

The genomecomb job system allows you to create workflows by writing and
debugging (easy) sequential code, which you can (optionally) make
distributed by indicating subtasks (jobs) and their dependencies in the
code using the "job" commmand.

Subtasks can be run sequentially, locally distributed (using a number of
worker processes) or distributed on a cluster (using sun grid engine). On
a cluster the program will run briefly, submitting all jobs with the
correct dependencies and then finishing (while the jobs still run).
Dependencies and targets are file based.

It also shares some features with make. It will not (re)run subtasks for
which the target (file) is already completed, and none of the dependencies
are newer. If the program is interupted, rerunning with the same
parameters will continue the analysis where it left off.
To avoid continuing on unfinished (interupted) files, write results to
temporary files during processing, and rename to the final target when
(successfully) finished

The job system is used by including the following commands (example in job_example.tcl):

== job_init ==
job_init initializes the job system. It accept the following job options
and returns the rest of the arguments:
; -d value (--distribute): if value is a number, the job is distributed over the given number of processes. 
Use sge to distribute using Grid Engine (also -distribute) or -d slurm to use the slurm job manager.
; -dpriority value: (only for sge) run jobs with the given priority (default is 0)
; -dqueue value: (only for sge) Use this option if you want to run on another queue than the default all.q
; -dcleanup success/never/allways: Allows you to keep all run/log files in the log_jobs
dirs even if a jub is successfull (**never**), or to allways delete them (**allways**)
instead of only deleting them after a succesfull run (**success**)
; -v number (--verbose): This general option has a big effect on job enabled commands:
If it is > 0, they will output a lot of info (to stder) on dependencies, jobs started, etc.
; -f 0/1 (--force): use -f 1 to rerun all jobs, even if some were actually already successfully finished (also -force)
; -silent 0/1: use -silent 1 to supress output with info on the processing
; -skipjoberrors 0/1: Errors in one job can cause the command to stop; 
use -skipjoberrors 1 to skip over these errors and complete other jobs (as far as they are not
dependent on the the job with an error)
; -runcmd cmd (-runcommand): For distributed processing, the command used to run job scripts made (default is "cg source -stack 1")
; -logfile filename: (base) filename of the logfile (see next paragraph)
; -dnosubmit 0/1: (debugging, only for sge, slurm) when set to 1 (default 0) Go over startup,
generating logging output, but without actually submitting jobs to the cluster. 
WARNING: Files older than dependencies are renamed to file.old
; -dry 0/1: (debugging, only for sge, slurm) when set to 1 (default 0) Go over startup,
generating logging output, but without actually submitting jobs to the cluster. 
Files older than dependencies are not renamed to file.old

Typical use:
{{{
set argv [job_init {*}$argv]
}}}

== job_logfile ==
job_logfile sets the (base) filename of the logfile. A timestamp and
changing extension (.submitting, .running, .finished, .error according to
status) will be added. The optional extra parameters are added as info in the comments
of the log file: **basedir** sets a base directory in which most of the action
will be going on. If jobfiles are within this directory, they are recorded as relative paths.
**cmdline** is used to indicate with which parameters the command/run was started.
Custom information can be added using extra key value pairs. (The keys
distribute and pid are reserved for use by the job system)

Typical use:
{{{
job_logfile logfile ?basedir? ?cmdline? ?key value? ... 
}}}

You can only have one logfile for a run, which is set by the first call to job_logfile.
Calling job_logfile a second time will not change this. (The -logfile option in job_init
will override a subsequent call to job_logfile)
set_job_logfile also sets the variable job_logdir to [file dir $logdir]/log_jobs
(or to any directory given as an argument)

== job_logdir ==
The job log directory is a directory where per job output, job scripts,
logs,  etc. will be stored while jobs are running. Each job is (should be)
uniquely defined by the combination of log dir and jobname (see further).
The job logdir can be changed (multiple times, resulting in multiple
logdirs) during a run, so that the same job(name)s can be used together
with a changed logdir for e.g. processing multiple samples using the same
code.

After a successfull run, useful info from the logdirs is collected into
the logfile and the logdirs are by default removed. After an error, they
are by default retained for error checking/solving.

The current job log directory is given by the (content of the) variable
job_logdir. The job command will give an error if job_logdir is not defined.
You can change the variable directly or you can set it using the
set_job_logdir command (which will also expand it to the full path and create
the directory), e.g.
{{{
set_job_logdir log_jobs
}}}
job_init will set log_jobs to a default value (log_jobs in the directory of the logfile).

== job ==
The job command is used to define a subtask that may be distributed and
to state dependencies (files the job uses) and targets (files the job will
create). In the program between job commands you can use any code, even
for giving some of the parameters to the job command. The limitation is
that this code should not rely on the results of previous jobs (which may
not be finished yet at this point if run e.g. with sge). The code in
another job however can rely on these results if the resulting file/target
is given as a dependency to this job.
The command is run as follows
{{{
job jobname ?options? -deps dependencylist -targets targetlist -code code
or
job jobname ?options? dependencylist targetlist code
}}}

**jobname** is the name given to the job, and has to be unique within the same log dir.
It is used to uniquely create run, log and error files in the log directory,
and to name jobs if run on a cluster.
The options -deps, -targets and -code are given in the command description as they
are obligatory. If not present as an option, the must be present as arguments.

**targetlist** contains a list of files that the job will create.
Variables are expanded in this list:
You can use -targets {$target1 $target2} and $target1 and
$target2 will be replaced with the value in variables target1 and target2.
When using this method, you should also enclose a one element list in {}:
If one of the variables used contains a space, the result would be split
up as separate elements.
You can of course also create a list explicitely using the list command
-targets [list $target1 $target2]

**dependencylist** contains a list of files which the job uses and is thus
dependent on: If one or more of these dependencies do not exist and are
not going to be made by previous jobs (targets), the program will stop
with an error. If the option -optional 1 is given, the job will be skipped
instead. Stating dependencies are important for parallel processing: A
job will only be run once the dependencies are available.

File names in the dependency list may use **wildcards**. These will be
expanded to all matching files, using both already existing files and files
that will be made by previous jobs (targets).

The list can also contain **optional dependencies**, indicated by a
filename (or pattern) enclosed in parentheses. The job will not give an
error or be skipped if an optional dependency does not exist, but the job
will not be actually executed until this dependency/file is made if it is
a target of a previous job.

**code** contains the code to be executed in the job. This code will run in a separate scope
from the code outside the job. Variables set in it will not be visible to the caller
and it does not see variables in the caller (though you can pass them using the -vars option).
locally defined procs (i.e. not automatically called from the path) are also not visible, unless 
passed with the -procs option
It has a number of predefined variables:
**deps** (contains the list of dependencies; in case you used wildcards,
deps will contain the filenames, not the patterns), 
targets (filenames to be created). 
For convenience are also available: 
dep (first dependency), dep2 (second dependency) and
target (first target), target2 (second target).
It is advised (generally as well) to create **result files using a temporary name**
(e.g. $target.temp), and rename them to $target when finished: This way, if a
job crashes, there will never be a target file present that is incomplete.

job supports following **options**:
; -deps dependencylist: list of files on which this job is dependent. 
; -targets targetlist: list of files that will be created by the job
; -code code: code to be executed in the job. 
; -vars varlist: a list of variables that must be passed on to the job.
; -procs proclist: a list of procs that must be passed on to the job.
; -optional 0/1: By default (0) an error will be given if the job cannot run because of missing
dependencies. Set -optional to 1 to indicate that this job is optional, and can be simply skipped
if not all dependencies are available.
; -skip skiplist: If all files in skiplist exist, the job will be skipped, 
even if some of the targets are not present. This can be used to avoid (re)running substeps
to a final goal  (where the files resulting from this substep are removed after the final result is made).
The -skip option can be given multiple times. The job is skipped if any of the given skiplists is complete.
; -force 0/1: If 1, this job will be run even if the target files already exist. This is usefull
if e.g. an existing files needs to be adapted.
; -checkcompressed 0/1: set to 1 to search dependencies (and targets) 
regardless of compression status (e.g. either test.tsv as
test.tsv.zst will match test.tsv). (default 0)
; -rmtargets rmtargets: This job will have the removal of given files as target
; -io io: give value > 1 if job is heavy on io (in sge, translates to (soft) job options: -l io=$value) 
; -cores numcores: give value > 1 if job uses more than 1 core (on sge hard option: "-pe local $value" if local pe is defined)
If using multiple cores, other requirements on sge are per core!
For memory (-mem) this conversion will be done by the system, and you must give the total required memory (for the job)
; -mem memorysize: indication of (maximum) memory needed by the job (on sge: -l mem_free=$value,virtual_free=$value, where value is corrected by the system based on -cores)
; -time timelimit: indication of (maximum) time needed by the job (on sge: -l s_rt=$time)
; -hard hardlimits: on sge: extra options passed on as hard limits
; -soft softlimits: on sge: extra options passed on as soft limits

== tools ==
jobglob: same as glob, but takes into account files that will be available
at that stage of the program (targets of previous jobs)
You can use this command to start a separate job for each file/dependency matching a pattern, e.g.
{{{
foreach file [jobglob test*.tsv] {
	job test-$file -deps {$file} -targets {[file root $file]-result.tsv} -code {
		# do something
	}
}
}}}

jobglob1: returns the first file in a jobglob

jobfileexists: same as "file exists", but takes into account files that will be available
at that stage of the program (targets of previous jobs)

jobtargetexists target dep1 ...: returns 1 if target already exists and is
newer than all dependencies. Returns 0 if target does not exists or if one
of the files/patterns in deps is newer than target or is in the queue or
being made. By default if a dep does not exist and is not being made, it
will not be considered "newer". You can use the -checkdepexists 1 option
to also return 0 if a dep does not exist.

job_cleanup_add file ?file? ..: add files to list of files that will be cleaned 
up after the run. Deletion will be done using -force, so if **file** is a directory,
it will be deleted even if there are still files in it

job_cleanup_ifempty_add file ?file? ..: add files to list of files that will be cleaned 
up after the run. Deletion will be done without -force, so if **file** is a directory,
it will only be deleted if empty. These are deleted after regular cleanup
(which should delete all files in the directory)

== job_runall ==
Jobs that will make targets that cannot (easily) be determined beforehand,
e.g. a number of output files with file names based on calculation, are
best avoided if these results must be processed further in the workflow:
Because you cannot specify the resulting files as targets, commands like
jobglob will not see them untill they are actually made, making this incompatible
with distributed jobs (which rely on dependencies to know when and which jobs can be run).

If you really cannot avoid this type of job, you can use the command
job_runall after it. job_runall will wait untill all jobs started up till
there are finished, and then continue the submission script. This works
because the targets are then actually made and can be (job)globbed. (This
does severily limits parallelisation.) For direct runs (-d direct), the
command has no effect (not needed, as all jobs are allways directly run).

== job_wait ==
job_wait must be added at the end of the program. It will wait (if needed) for all jobs to finish.
Then it will cleanup temporary files and update the log file.
On a cluster (sge) the program will actually finish; The final processing by job_wait is submitted as a job that
will run after all other jobs are finished.

{{{
job_wait
}}}

== Category ==
Dev

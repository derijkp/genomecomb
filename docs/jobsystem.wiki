= Job system =

The job system in genomecomb allows to write distributed code in a
seemingly sequential way, by indicating subtasks (jobs) and their
dependencies. Subtasks can be run sequentially, locally distributed
(using a number of worker processes) or distributed on a cluster (using
sun grid engine). On a cluster the program will run briefly, submitting
all jobs with the correct dependencies and then finishing (while the jobs
still run). Dependencies and targets are file based.

It also shares some features with make. It will not (re)run subtasks for
which the target (file) is already completed, and none of the dependencies
are newer. If the program is interupted, rerunning with the same
parameters will continue the analysis where it left off.

The job system is used by including the following commands (example in job_example.tcl):

== job_init ==
job_init initializes the job system. It accept the following job options
and returns the rest of the arguments:
; -d value (--distribute): if value is a number, the job is distributed over the given number of processes. Use sge to distribute using Grid Engine  (also -distribute)
; -dpriority value: (only for sge) run jobs with the given priority (default is 0)
; -dqueue value: (only for sge) Use this option if you want to run on another queue than the default all.q
; -dcleanup success/never/allways: Allows you to keep all run/log files in the log_jobs
dirs even if a jub is successfull (**never**), or to allways delete them (**allways**)
instead of only deleting them after a succesfull run (**success**)
; -v number (--verbose): This general option has a big effect on job enabled commands:
If it is > 0, they will output a lot of info (to stder) on dependencies, jobs started, etc.
; -f 0/1 (--force): use -f 1 to rerun all jobs, even if some were actually already successfully finished (also -force)
; --silent 0/1: use -silent 1 to supress output with info on the processing
; --skipjoberrors 0/1: Errors in one job can cause the command to stop; 
use --skipjoberrors 1 to skip over these errors and complete other jobs (as far as they are not
dependent on the the job with an error)
; --runcmd cmd (--runcommand): For distributed processing, the command used to run job scripts made (default is "cg source")

Typical use:
{{{
set argv [job_init {*}$argv]
}}}

== job_logfile ==
job_logfile sets the (base) filename of the logfile. A timestamp and
changing extension (.submitting, .running, .finished, .error according to
status) will be added. The optional extra parameters are added as info in the comments
of the log file: **basedir** sets a base directory in which most of the action
will be going on. If jobfiles are within this directory, they are recorded as relative paths.
**cmdline** is used to indicate with which parameters the command/run was started.
Custom information can be added using extra key value pairs. (The keys
distribute and pid are reserved for use by the job system)

Typical use:
{{{
job_logfile logfile ?basedir? ?cmdline? ?key value? ... 
}}}

== job_logdir ==
job_logdir sets the current job log directory.
This is a directory where output, job scripts etc. will be stored.
A job is (should) be uniquely defined by the combination of log dir and
jobname (see further)
You can us different log dirs in one run (change the log dir during the
program using the job_logdir command), e.g.
{{{
job_logdir log_jobs
}}}

== job ==
The job command is used to define a subtask that may be distributed and
to state dependencies (files the job uses) and targets (files the job will
create). In the program between job commands you can use any code, even
for giving some of the parameters to the job command. The limitation is
that this code should not rely on the results of previous jobs (which may
not be finished yet at this point if run e.g. with sge). The code in
another job however can rely on these results if the resulting file/target
is given as a dependency to this job.
The command is run as follows
{{{
job jobname ?options? -deps dependencylist -targets targetlist -code code
or
job jobname ?options? dependencylist targetlist code
}}}
**jobname** is the name given to the job, and has to be unique within the same log dir.
It is used to uniquely create run, log and error files in the log directory,
and to name jobs if run on a cluster.
The options -deps, -targets and -code are given in the command description as they
are obligatory. If not present as an option, the must be present as arguments.

Supported options:
; **-deps dependencylist**: list of files on which this job is dependent. 
If one of these is not present, the job will be skipped. 
File names in the dependency list may use **wildcards**. These will be
expanded to all matching files, using both already existing files and files
that will be made by previous jobs (targets).
The elements of the list are evaluated, so variables are expanded.
The list can also contain **optional dependencies**, indicated by a
filename(pattern) enclosed in parentheses. The job will not be skipped if
an optional dependency does not exist, but the job will not be actually
executed until this dependency/file is made.
; **-targets targetlist**: files that will be created by the job
; **-code code**: code to be executed in the job. This code will run in a separate scope
from the code outside the job. Variables set in it will not be visible to the caller
and it does not see variables in the caller (though you can pass them using the -vars option).
It has a number of predefined variables:
deps (containing the list of dependencies; in case you used wildcards,
deps will contain the filenames, not the patterns), 
targets (filenames to be created). For convenience are also available: 
dep (first dependency), dep2 (second dependency) and
target (first target), target2 (second target).
It is advised (generally as well) to create **result files using a temporary name**
(e.g. $target.temp), and rename them to $target when finished: This way, if a
job crashes, there will never be a target file present that is incomplete.
; -vars varlist: a list of variables that must be passed on to the job.
; -optional 0/1: By default (0) an error will be given if the job cannot run because of missing
dependencies. Set -optional to 1 to indicate that this job is optional, and can be simply skipped
if not all dependencies are available.
; -skip skiplist: If all files in skiplist exist, the job will be skipped, 
even if some of the targets are not present. This can be used to avoid (re)running substeps
to a final goal.
; -force 0/1: If 1, this job will be run even if the target files already exist. This is usefull
if e.g. an existing files needs to be adapted.
; -checkcompressed 0/1: dependencies (and targets) are normally
searched regardless of compression status (e.g. either test.tsv as
test.tsv.lz4 will match test.tsv). Use this option for the job 
specifically targetting compression
; -foreach pattern: A separate job will be started for each dependency matching the pattern.
This can also be handled (more flexibly) using jobglob and a foreach loop
{{{
foreach file [jobglob test*.tsv] {
	job test-$file -deps $file -targets [file root $file]-result.tsv -code {
	}
}
}}}
; -rmtargets rmtargets: This job will have the removal of given files as target
; -ptargets patterntargets: avoid the use of this option; it only works on local distribution.
Here it can be used when a job will make targets that cannot (easily) be determined when
the (submission) program is run. The (p)targets can be given using a wildcard pattern matching all output.
On local distribution, all jobs with dependencies matching the pattern are postponed until
this job is done. Distribution on a cluster is not compatible with this model as there all jobs 
are submitted in one go with dependencies between them given to the cluster managment system.
(You could run a program using this option on a cluster by rerunning the submission program
after the ptargets job is finished.)
; -io io: give value > 1 if job is heavy on io (in sge, translates to (soft) job options: -l io=$value) 
; -cores numcores: give value > 1 if job uses more than 1 core (on sge hard option: "-pe local $value" if local_pe is defined)
; -mem memorysize: indication of memory needed (on sge (hard): -l mem_free=$value)
; -hard hardlimits: on sge: extra options passed on as hard limits
; -soft softlimits: on sge: extra options passed on as soft limits

== tools ==
jobglob: same as glob, but takes into account files that will be available
at that stage of the program (targets of previous jobs)

jobglob1: returns the first file in a jobglob

jobfileexists: same as "file exists", but takes into account files that will be available
at that stage of the program (targets of previous jobs)

jobtargetexists target dep1 ...: returns 1 if target already exists and is
newer than all dependencies. Returns 0 if target does not exists or if one
of the files/patterns in deps is newer than target or is in the queue or
being made. By default if a dep does not exist and is not being made, it
will not be considered "newer". You can use the -checkdepexists 1 option
to also return 0 if a dep does not exist.

== job_wait ==
job_wait must be added at the end of the program. It will wait (if needed) for all jobs to finish.
This does not actually do something for direct runs (jobs are executed directly) or on a cluster
(sge submits jobs directly with the needed dependencies), but is required for the proper
functioning of locally distributed processing using workers (-d number).
Here it will manage the processes.

{{{
job_wait
}}}

== Category ==
dev
